{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71b043f"
      },
      "source": [
        "# **What is Tokenization?**\n",
        "\n",
        "Tokenization is a fundamental initial step in Natural Language Processing (NLP) that involves segmenting a continuous stream of text into smaller, meaningful units called \"tokens.\" These tokens can be individual words, numbers, punctuation marks, or even subword units, depending on the specific tokenizer used and the context of the NLP task. The primary goal of tokenization is to break down raw text into a structured sequence that can be more easily processed and analyzed by algorithms, serving as the groundwork for subsequent NLP operations such as parsing, text analysis, and machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NNK_jt-zh7NI"
      },
      "outputs": [],
      "source": [
        "!pip install --q nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AXxHydKli_3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDGQghTjh7NQ",
        "outputId": "9475ee1c-d665-40b3-c968-5b151d694fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Tokenization Explained Step-by-Step ###\n",
            "\n",
            "--- Example 1: Sentence Tokenization ---\n",
            "Original Text:\n",
            "Natural Language Processing is an exciting field. It combines computer science, artificial intelligence, and linguistics. Many applications benefit from NLP, such as machine translation and sentiment analysis.\n",
            "\n",
            "Tokenized Sentences:\n",
            "Sentence 1: 'Natural Language Processing is an exciting field.'\n",
            "Sentence 2: 'It combines computer science, artificial intelligence, and linguistics.'\n",
            "Sentence 3: 'Many applications benefit from NLP, such as machine translation and sentiment analysis.'\n",
            "Explanation: `sent_tokenize` effectively identifies sentence boundaries, often based on punctuation like periods, question marks, and exclamation marks. Each string in the output list represents a complete sentence.\n",
            "\n",
            "--- Example 2: Word Tokenization (Standard) ---\n",
            "Original Text:\n",
            "I love learning about NLP! It's truly fascinating.\n",
            "\n",
            "Tokenized Words:\n",
            "['I', 'love', 'learning', 'about', 'NLP', '!', 'It', \"'s\", 'truly', 'fascinating', '.']\n",
            "Explanation: `word_tokenize` splits the text into words and punctuation marks. Notice how 'NLP!' is split into 'NLP' and '!' and 'It's' is split into 'It', ''s'. This method handles most contractions and punctuation quite well.\n",
            "\n",
            "--- Example 3: Word Tokenization (TreebankWordTokenizer) ---\n",
            "Original Text:\n",
            "Don't forget to 'code' in Colab; it's so much fun!\n",
            "\n",
            "Tokenized Words (Treebank):\n",
            "['Do', \"n't\", 'forget', 'to', \"'code\", \"'\", 'in', 'Colab', ';', 'it', \"'s\", 'so', 'much', 'fun', '!']\n",
            "Explanation: The `TreebankWordTokenizer` uses a set of rules from the Penn Treebank project. It often handles contractions ('Don't' -> ['Do', \"n't\"], 'it's' -> ['it', \"'s\"]) and punctuation (like apostrophes in 'code') in a specific, consistent manner, which can be beneficial for certain parsing tasks. Compared to `word_tokenize`, its handling of some edge cases, especially contractions and certain punctuation, can be more granular or linguistically motivated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to resolve the LookupError\n",
        "\n",
        "print(\"### Tokenization Explained Step-by-Step ###\")\n",
        "\n",
        "# Tokenization is the process of breaking down a text into smaller units called tokens.\n",
        "# These tokens can be words, phrases, or even whole sentences, depending on the type of tokenization.\n",
        "# It's a fundamental step in most NLP tasks.\n",
        "\n",
        "\n",
        "## Example 1: Sentence Tokenization\n",
        "# Goal: Divide a paragraph or document into individual sentences.\n",
        "# NLTK's `sent_tokenize` is excellent for this.\n",
        "\n",
        "print(\"\\n--- Example 1: Sentence Tokenization ---\")\n",
        "# Define a new example corpus for sentence tokenization\n",
        "sentence_corpus = \"\"\"Natural Language Processing is an exciting field. It combines computer science, artificial intelligence, and linguistics. Many applications benefit from NLP, such as machine translation and sentiment analysis.\"\"\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(sentence_corpus)\n",
        "\n",
        "# Step 1: Import the necessary function\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Step 2: Apply sentence tokenization\n",
        "sentences = sent_tokenize(sentence_corpus)\n",
        "\n",
        "# Step 3: Print and explain the output\n",
        "print(\"\\nTokenized Sentences:\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}: '{sent}'\")\n",
        "print(\"Explanation: `sent_tokenize` effectively identifies sentence boundaries, often based on punctuation like periods, question marks, and exclamation marks. Each string in the output list represents a complete sentence.\")\n",
        "\n",
        "\n",
        "## Example 2: Word Tokenization (Standard)\n",
        "# Goal: Divide a sentence or text into individual words.\n",
        "# NLTK's `word_tokenize` is a common and versatile choice.\n",
        "\n",
        "print(\"\\n--- Example 2: Word Tokenization (Standard) ---\")\n",
        "# Define a new example sentence for word tokenization\n",
        "word_corpus_1 = \"I love learning about NLP! It's truly fascinating.\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(word_corpus_1)\n",
        "\n",
        "# Step 1: Import the necessary function\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Step 2: Apply word tokenization\n",
        "words_1 = word_tokenize(word_corpus_1)\n",
        "\n",
        "# Step 3: Print and explain the output\n",
        "print(\"\\nTokenized Words:\")\n",
        "print(words_1)\n",
        "print(\"Explanation: `word_tokenize` splits the text into words and punctuation marks. Notice how 'NLP!' is split into 'NLP' and '!' and 'It's' is split into 'It', ''s'. This method handles most contractions and punctuation quite well.\")\n",
        "\n",
        "\n",
        "## Example 3: Word Tokenization (TreebankWordTokenizer)\n",
        "# Goal: Divide a sentence into words using a tokenizer specifically trained on Penn Treebank data.\n",
        "# This tokenizer often has specific rules for contractions and punctuation that differ slightly from `word_tokenize`.\n",
        "\n",
        "print(\"\\n--- Example 3: Word Tokenization (TreebankWordTokenizer) ---\")\n",
        "# Define an example sentence to highlight Treebank specific tokenization\n",
        "word_corpus_2 = \"Don't forget to 'code' in Colab; it's so much fun!\"\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(word_corpus_2)\n",
        "\n",
        "# Step 1: Import and instantiate the tokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "# Step 2: Apply Treebank word tokenization\n",
        "words_2 = treebank_tokenizer.tokenize(word_corpus_2)\n",
        "\n",
        "# Step 3: Print and explain the output\n",
        "print(\"\\nTokenized Words (Treebank):\")\n",
        "print(words_2)\n",
        "print(\"Explanation: The `TreebankWordTokenizer` uses a set of rules from the Penn Treebank project. It often handles contractions ('Don't' -> ['Do', \\\"n't\\\"], 'it's' -> ['it', \\\"'s\\\"]) and punctuation (like apostrophes in 'code') in a specific, consistent manner, which can be beneficial for certain parsing tasks. Compared to `word_tokenize`, its handling of some edge cases, especially contractions and certain punctuation, can be more granular or linguistically motivated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S4_z_iPDh7NL"
      },
      "outputs": [],
      "source": [
        "corpus=\"\"\"Hello Welcome,to Krish Naik's NLP Tutorials.\n",
        "Please do watch the entire course! to become expert in NLP.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PakMZKmuh7NM",
        "outputId": "8365a4bb-4021-4c00-c216-ecf5125b96ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to Krish Naik's NLP Tutorials.\n",
            "Please do watch the entire course! to become expert in NLP.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MVjdW6X6h7NM"
      },
      "outputs": [],
      "source": [
        "##  Tokenization\n",
        "## Sentence-->paragraphs\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FETATONbh7NN"
      },
      "outputs": [],
      "source": [
        "documents=sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPKd7SLhh7NN",
        "outputId": "81282626-83ed-4b44-9295-d3c7bfc9b2b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "type(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAlJAbj-h7NO",
        "outputId": "6a7a4633-3a0e-49d3-b05e-47e1c7d17dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to Krish Naik's NLP Tutorials.\n",
            "Please do watch the entire course!\n",
            "to become expert in NLP.\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDdpLaUQh7NO"
      },
      "outputs": [],
      "source": [
        "## Tokenization\n",
        "## Paragraph-->words\n",
        "## sentence--->words\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWgOH7Clh7NP",
        "outputId": "6a877891-23ae-41d9-a25d-d50d68c4f94e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Krish',\n",
              " 'Naik',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iXVJW4hh7NP",
        "outputId": "3b66fe3c-c58e-45fa-8842-188743de5d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'Welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials', '.']\n",
            "['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
            "['to', 'become', 'expert', 'in', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "for sentence in documents:\n",
        "    print(word_tokenize(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6spz5yoh7NP"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D-FuXDKh7NP",
        "outputId": "48a0dfb0-1e7e-40f5-8cce-a4a3cee18ae5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Krish',\n",
              " 'Naik',\n",
              " \"'\",\n",
              " 's',\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wordpunct_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy5Prfx4h7NQ"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVRIeVYZh7NQ"
      },
      "outputs": [],
      "source": [
        "tokenizer=TreebankWordTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMZSDwIah7NQ",
        "outputId": "1f24b3bd-8e74-4be4-99ac-58b19fa6e98c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Krish',\n",
              " 'Naik',\n",
              " \"'s\",\n",
              " 'NLP',\n",
              " 'Tutorials.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu2nFCnCh7NQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pb3eqSLh7NQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}