{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd8JH2ZC2hCj"
      },
      "source": [
        "## Wordnet Lemmatizer\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OOfmX-Dp2hCl"
      },
      "outputs": [],
      "source": [
        "## Q&A,chatbots,text summarization\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UVRs7syY2hCm"
      },
      "outputs": [],
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Peapr1xH2hCm",
        "outputId": "4ab887e2-74d6-4080-a530-811d0e731b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\",pos='v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JeFjOqZc2hCn"
      },
      "outputs": [],
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXkIQQsC2hCn",
        "outputId": "a919f335-ff1e-4067-a724-23b10a4c6346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XHP8sSut2hCn",
        "outputId": "c6870d50-fc0d-44ff-c3c6-2ee416810cbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"goes\",pos='v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doab5D852hCo",
        "outputId": "9be22be4-70b9-4d8c-cd6a-c14839e93367"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "lemmatizer.lemmatize(\"fairly\",pos='v'),lemmatizer.lemmatize(\"sportingly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12b46204"
      },
      "source": [
        "## Explain Lemmatization and Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de3c5196"
      },
      "source": [
        "### What is Lemmatization?\n",
        "Lemmatization is a linguistic process in Natural Language Processing (NLP) that reduces words to their base or dictionary form, known as a 'lemma'. Unlike stemming, which often chops off suffixes to get to a root form (which might not be a real word), lemmatization uses vocabulary and morphological analysis to return a valid word.\n",
        "\n",
        "For example:\n",
        "*   'running', 'runs', 'ran' all reduce to the lemma 'run'.\n",
        "*   'better', 'best' reduce to 'good'.\n",
        "\n",
        "### Importance in Text Preprocessing\n",
        "Lemmatization is crucial in text preprocessing for several reasons:\n",
        "1.  **Semantic Consistency**: It ensures that different inflected forms of a word are treated as the same item, which is vital for understanding the meaning of text.\n",
        "2.  **Reduced Vocabulary Size**: By consolidating variations of words into a single lemma, it significantly reduces the number of unique words in a corpus. This helps in more efficient storage and processing.\n",
        "3.  **Improved NLP Task Accuracy**: For tasks like text classification, sentiment analysis, information retrieval, and machine translation, lemmatization helps improve accuracy by ensuring that the model doesn't treat different forms of the same word as distinct entities.\n",
        "4.  **Handling Word Variations**: It effectively handles irregular word forms (e.g., 'went' to 'go', 'are' to 'be'), which stemming often fails to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "318bf16e"
      },
      "source": [
        "### Lemmatization vs. Stemming\n",
        "While both lemmatization and stemming aim to reduce inflected words to a common base form, they differ significantly in their approach and the quality of their output:\n",
        "\n",
        "*   **Stemming**: This is a more rudimentary process that typically involves chopping off suffixes from words. It's faster and simpler but often results in 'stems' that are not actual words. For example, 'consultant', 'consulting', 'consultants' might all be reduced to 'consult'. This stem is not always a valid word in itself and might lose some meaning.\n",
        "    *   *Example:* 'beautiful' -> 'beauti', 'connection' -> 'connect'\n",
        "\n",
        "*   **Lemmatization**: This is a more sophisticated process that uses a vocabulary and morphological analysis to return the base or dictionary form of a word, which is always a valid word (a lemma). It considers the context and part of speech of the word to ensure accuracy.\n",
        "    *   *Example:* 'better' -> 'good', 'going' -> 'go'\n",
        "\n",
        "**Key Differences:**\n",
        "*   **Output**: Lemmatization produces a valid word (lemma); stemming produces a root form (stem) which may not be a valid word.\n",
        "*   **Approach**: Lemmatization uses dictionaries and morphological rules; stemming uses heuristic rules to remove suffixes.\n",
        "*   **Complexity**: Lemmatization is generally more complex and computationally intensive than stemming.\n",
        "*   **Accuracy**: Lemmatization is typically more accurate and linguistically correct, especially for irregular forms.\n",
        "*   **Use Cases**: Stemming is often used in information retrieval where speed is critical and perfect accuracy is not paramount. Lemmatization is preferred for NLP tasks where semantic understanding and accuracy are more important, such as question answering, machine translation, and text summarization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c289a275"
      },
      "source": [
        "### Overview of Text Preprocessing Steps\n",
        "Text preprocessing is a critical phase in NLP to clean and prepare raw text data for analysis and model training. It involves several common steps, which can vary based on the specific NLP task and dataset:\n",
        "\n",
        "1.  **Tokenization**: This is the process of breaking down a stream of text into smaller units called tokens. Tokens can be words, subwords, or even characters, depending on the granularity required. For example, the sentence \"Hello, world!\" might be tokenized into [\"Hello\", \",\", \"world\", \"!\"] or [\"Hello\", \"world\"].\n",
        "2.  **Lowercasing**: Converting all text to lowercase helps ensure that the same word with different capitalizations (e.g., \"Apple\" vs. \"apple\") is treated as a single token, reducing vocabulary size and improving consistency.\n",
        "3.  **Removing Punctuation and Special Characters**: Punctuation marks (like '.', ',', '!', '?') and other special characters (like '@', '#', '$') often do not carry significant semantic meaning for many NLP tasks and can be removed to reduce noise.\n",
        "4.  **Stop Word Removal**: Stop words are common words (e.g., \"the\", \"a\", \"is\", \"in\") that appear frequently in a language but usually add little to the meaning of a sentence. Removing them can reduce dimensionality and focus on more important terms.\n",
        "5.  **Lemmatization/Stemming**: As discussed, this step reduces words to their base or root form. Lemmatization is generally preferred for its accuracy in producing valid words, but stemming can be used when computational efficiency is a higher priority.\n",
        "6.  **Removing Numbers**: Depending on the task, numbers may or may not be relevant. For tasks like sentiment analysis, numbers might be removed or replaced.\n",
        "7.  **Removing Whitespace**: Excess spaces, tabs, and newlines can be cleaned up to standardize the text.\n",
        "8.  **Handling Emojis/Emoticons**: Emojis can carry sentiment and might need to be processed (e.g., converted to text descriptions) or removed, depending on the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47020ac3"
      },
      "source": [
        "## Lemmatization with POS Tags\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a02ea5f3"
      },
      "source": [
        "### The Crucial Role of POS Tags in Lemmatization\n",
        "\n",
        "Providing the correct Part-of-Speech (POS) tag is crucial for accurate lemmatization. The same word can have different base forms (lemmas) depending on its grammatical role (e.g., as a verb, noun, or adjective). Without a specified POS tag, `WordNetLemmatizer` typically defaults to assuming the word is a noun, which can lead to incorrect lemmas if the word is intended as a verb or adjective.\n",
        "\n",
        "Let's illustrate this with examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27ba4dbd",
        "outputId": "a90b3c56-4b1c-4ed4-a0ac-aef63d79e67f"
      },
      "source": [
        "print(f\"'better' as adjective (pos='a'): {lemmatizer.lemmatize('better', pos='a')}\")\n",
        "print(f\"'better' without POS tag (defaults to noun): {lemmatizer.lemmatize('better')}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'better' as adjective (pos='a'): good\n",
            "'better' without POS tag (defaults to noun): better\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccc96507",
        "outputId": "2e99fc25-e264-4722-ef36-3b3f36bac51c"
      },
      "source": [
        "print(f\"'meeting' as noun (pos='n'): {lemmatizer.lemmatize('meeting', pos='n')}\")\n",
        "print(f\"'meeting' as verb (pos='v'): {lemmatizer.lemmatize('meeting', pos='v')}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'meeting' as noun (pos='n'): meeting\n",
            "'meeting' as verb (pos='v'): meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7a11b7d",
        "outputId": "4fe812c7-b562-4ce8-dd90-373bebbf3c1a"
      },
      "source": [
        "print(f\"'bat' as noun (pos='n'): {lemmatizer.lemmatize('bat', pos='n')}\")\n",
        "print(f\"'bat' as verb (pos='v'): {lemmatizer.lemmatize('bat', pos='v')}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'bat' as noun (pos='n'): bat\n",
            "'bat' as verb (pos='v'): bat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc3d8069"
      },
      "source": [
        "### Explanation of Examples:\n",
        "\n",
        "**1. 'better' as adjective vs. default:**\n",
        "*   When `lemmatizer.lemmatize('better', pos='a')` is used, the word 'better' (an adjective) is correctly lemmatized to 'good', which is its base form when used in comparison.\n",
        "*   When `lemmatizer.lemmatize('better')` is used without a `pos` tag, it defaults to `pos='n'` (noun). Since 'better' is not typically a noun with a different lemma, it remains 'better'. This shows that without the correct POS tag, the lemmatizer might not find the intended lemma.\n",
        "\n",
        "**2. 'meeting' as noun vs. verb:**\n",
        "*   As a noun (`pos='n'`), 'meeting' is already in its base form (e.g., \"a meeting\"), so it remains 'meeting'.\n",
        "*   As a verb (`pos='v'`), 'meeting' (from \"they are meeting\") is correctly lemmatized to its base verb form, 'meet'. This clearly illustrates how the same word form can have different lemmas depending on its grammatical context.\n",
        "\n",
        "**3. 'bat' as noun vs. verb:**\n",
        "*   In both cases, 'bat' as a noun (`pos='n'`) and 'bat' as a verb (`pos='v'`) lemmatizes to 'bat'. This is an interesting case where the lemma for different POS tags happens to be the same base word. However, it still demonstrates the process of the lemmatizer checking against the specified POS category.\n",
        "\n",
        "These examples collectively demonstrate that providing the correct POS tag significantly enhances the accuracy of lemmatization by guiding the `WordNetLemmatizer` to the appropriate morphological analysis within the WordNet corpus. Without it, the lemmatizer relies on its default assumption (usually a noun), which may not always yield the desired or correct lemma for words used in other grammatical roles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8cdb4d1"
      },
      "source": [
        "## Demonstrate Basic Lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da3a51cb",
        "outputId": "2fcf78a8-b31d-4a86-c05c-6e22b1945556"
      },
      "source": [
        "basic_words = [\"dogs\", \"geese\", \"leaves\", \"running\", \"amazed\"]\n",
        "\n",
        "print(\"Lemmatization with default POS (noun):\")\n",
        "for word in basic_words:\n",
        "    print(f\"{word} ----> {lemmatizer.lemmatize(word)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization with default POS (noun):\n",
            "dogs ----> dog\n",
            "geese ----> goose\n",
            "leaves ----> leaf\n",
            "running ----> running\n",
            "amazed ----> amazed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d64400a"
      },
      "source": [
        "### Explanation of Basic Lemmatization Output\n",
        "\n",
        "The output from the basic lemmatization example clearly illustrates the default behavior of `WordNetLemmatizer` when no Part-of-Speech (POS) tag is explicitly provided. By default, the lemmatizer assumes the word is a **noun** (`pos='n'`).\n",
        "\n",
        "Let's break down the examples:\n",
        "\n",
        "*   **`dogs` ----> `dog`**: Correctly lemmatized a plural noun to its singular form.\n",
        "*   **`geese` ----> `goose`**: Correctly lemmatized an irregular plural noun to its singular form.\n",
        "*   **`leaves` ----> `leaf`**: Correctly lemmatized a plural noun (referring to plant leaves) to its singular form.\n",
        "*   **`running` ----> `running`**: Here, `running` is often a verb or adjective. Since the lemmatizer defaults to noun, and 'running' is not a noun with a different base form (e.g., as in 'a running track'), it remains unchanged. If `pos='v'` were provided, it would likely become `run`.\n",
        "*   **`amazed` ----> `amazed`**: Similar to 'running', 'amazed' is typically a verb (past participle) or an adjective. As a noun, it doesn't have a distinct base form, so it remains unchanged. If `pos='v'` were provided, it would likely become `amaze`.\n",
        "\n",
        "This demonstrates that while the default behavior works well for typical plural nouns, it might not yield the desired lemma for words that are primarily verbs, adjectives, or adverbs, as it will incorrectly assume they are nouns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbb87f1a"
      },
      "source": [
        "### Explanation of Basic Lemmatization Output\n",
        "\n",
        "The output from the basic lemmatization example clearly illustrates the default behavior of `WordNetLemmatizer` when no Part-of-Speech (POS) tag is explicitly provided. By default, the lemmatizer assumes the word is a **noun** (`pos='n'`).\n",
        "\n",
        "Let's break down the examples:\n",
        "\n",
        "*   **`dogs` ----> `dog`**: Correctly lemmatized a plural noun to its singular form.\n",
        "*   **`geese` ----> `goose`**: Correctly lemmatized an irregular plural noun to its singular form.\n",
        "*   **`leaves` ----> `leaf`**: Correctly lemmatized a plural noun (referring to plant leaves) to its singular form.\n",
        "*   **`running` ----> `running`**: Here, `running` is often a verb or adjective. Since the lemmatizer defaults to noun, and 'running' is not a noun with a different base form (e.g., as in 'a running track'), it remains unchanged. If `pos='v'` were provided, it would likely become `run`.\n",
        "*   **`amazed` ----> `amazed`**: Similar to 'running', 'amazed' is typically a verb (past participle) or an adjective. As a noun, it doesn't have a distinct base form, so it remains unchanged. If `pos='v'` were provided, it would likely become `amaze`.\n",
        "\n",
        "This demonstrates that while the default behavior works well for typical plural nouns, it might not yield the desired lemma for words that are primarily verbs, adjectives, or adverbs, as it will incorrectly assume they are nouns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9140f399"
      },
      "source": [
        "## Full Text Preprocessing Example with Lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d853137",
        "outputId": "1aac7762-b6f9-404b-a7a0-661e9eae1d71"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"The quick brown foxes are running quickly through the beautiful green forests.\"\n",
        "print(f\"Original Text: {text}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown foxes are running quickly through the beautiful green forests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e70472aa"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous series of fixes successfully addressed all `LookupError`s related to NLTK resource downloads and demonstrated the complete text preprocessing pipeline as requested. This step confirms and presents the working code for the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c75e92af",
        "outputId": "fa8804d4-3666-4b09-c180-2eccd84b4d1e"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to resolve the LookupError for punkt_tab\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Corrected typo from 'ltk' to 'nltk'\n",
        "tokenized_words = word_tokenize(text)\n",
        "print(f\"Tokenized Words: {tokenized_words}\")\n",
        "\n",
        "pos_tags_nltk = pos_tag(tokenized_words)\n",
        "print(f\"POS Tags (NLTK): {pos_tags_nltk}\")\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return 'a'  # Adjective\n",
        "    elif tag.startswith('V'):\n",
        "        return 'v'  # Verb\n",
        "    elif tag.startswith('N'):\n",
        "        return 'n'  # Noun\n",
        "    elif tag.startswith('R'):\n",
        "        return 'r'  # Adverb\n",
        "    else:\n",
        "        return 'n'  # Default to noun\n",
        "\n",
        "lemmatized_words = []\n",
        "for word, tag in pos_tags_nltk:\n",
        "    wordnet_pos = get_wordnet_pos(tag)\n",
        "    lemmatized_words.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
        "\n",
        "print(f\"Lemmatized Words: {lemmatized_words}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words: ['The', 'quick', 'brown', 'foxes', 'are', 'running', 'quickly', 'through', 'the', 'beautiful', 'green', 'forests', '.']\n",
            "POS Tags (NLTK): [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('are', 'VBP'), ('running', 'VBG'), ('quickly', 'RB'), ('through', 'IN'), ('the', 'DT'), ('beautiful', 'JJ'), ('green', 'JJ'), ('forests', 'NNS'), ('.', '.')]\n",
            "Lemmatized Words: ['The', 'quick', 'brown', 'fox', 'be', 'run', 'quickly', 'through', 'the', 'beautiful', 'green', 'forest', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac6c0e2f",
        "outputId": "e26b9579-e520-48ff-cfc4-63a839ba7d21"
      },
      "source": [
        "print(\"Text preprocessing pipeline demonstrated successfully, including tokenization, POS tagging, and lemmatization.\")\n",
        "print(f\"Final Lemmatized Words: {lemmatized_words}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing pipeline demonstrated successfully, including tokenization, POS tagging, and lemmatization.\n",
            "Final Lemmatized Words: ['The', 'quick', 'brown', 'fox', 'be', 'run', 'quickly', 'through', 'the', 'beautiful', 'green', 'forest', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "224a21f4"
      },
      "source": [
        "## Generate a Report/Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "203ece2a"
      },
      "source": [
        "\n",
        "### Q&A\n",
        "The task aimed to provide a comprehensive understanding and practical demonstration of lemmatization within the context of text preprocessing.\n",
        "\n",
        "*   **What is Lemmatization?**\n",
        "    Lemmatization is a linguistic process in Natural Language Processing (NLP) that reduces words to their base or dictionary form (lemma) using vocabulary and morphological analysis. It ensures semantic consistency, reduces vocabulary size, and improves NLP task accuracy by consolidating word variations.\n",
        "\n",
        "*   **How does Lemmatization differ from Stemming?**\n",
        "    Lemmatization produces a valid dictionary word (lemma) by considering context and part of speech, making it more accurate and linguistically correct, especially for irregular forms. Stemming, conversely, is a cruder process that chops off suffixes, often resulting in a root form that may not be a real word. While stemming is faster, lemmatization is preferred for tasks requiring higher semantic understanding.\n",
        "\n",
        "*   **How do POS tags affect Lemmatization?**\n",
        "    Providing the correct Part-of-Speech (POS) tag is crucial for accurate lemmatization. The `WordNetLemmatizer` defaults to assuming a word is a noun if no POS tag is specified. This can lead to incorrect lemmas if the word functions as a verb, adjective, or adverb. Examples like 'better' (adjective -> 'good') and 'meeting' (verb -> 'meet', noun -> 'meeting') clearly demonstrate how different POS tags for the same word form can yield different, more accurate lemmas.\n",
        "\n",
        "*   **What does a full text preprocessing pipeline with lemmatization look like?**\n",
        "    A complete pipeline typically involves several steps: Tokenization (breaking text into words), Lowercasing, Removing Punctuation/Special Characters, Stop Word Removal, POS Tagging, and finally Lemmatization using the POS tags. This sequence ensures that words are correctly identified and reduced to their base forms for further analysis.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Lemmatization Fundamentals**:\n",
        "    *   Lemmatization reduces words to their base form (lemma), for example, 'running', 'runs', 'ran' all reduce to 'run', and 'better', 'best' reduce to 'good'.\n",
        "    *   It's critical for semantic consistency, vocabulary reduction, and improving NLP task accuracy, especially for irregular word forms ('went' to 'go').\n",
        "    *   It differs from stemming in that it produces valid words using linguistic analysis, whereas stemming often produces non-words by merely chopping off suffixes.\n",
        "*   **Default Lemmatization Behavior (without POS tags)**:\n",
        "    *   When no Part-of-Speech (POS) tag is provided, the `WordNetLemmatizer` defaults to treating the word as a **noun**.\n",
        "    *   This default works effectively for plural nouns (e.g., 'dogs' $\\rightarrow$ 'dog', 'geese' $\\rightarrow$ 'goose', 'leaves' $\\rightarrow$ 'leaf').\n",
        "    *   However, for words that are primarily verbs or adjectives, the default behavior can be inaccurate; 'running' and 'amazed' remained unchanged because they were not recognized as nouns with different base forms.\n",
        "*   **Impact of POS Tags on Lemmatization Accuracy**:\n",
        "    *   Explicitly providing the correct POS tag is essential for accurate lemmatization.\n",
        "    *   The word 'better' lemmatizes to 'good' when tagged as an adjective (`pos='a'`), but remains 'better' without a specified POS tag (defaulting to noun).\n",
        "    *   The word 'meeting' lemmatizes to 'meet' when tagged as a verb (`pos='v'`) but remains 'meeting' when tagged as a noun (`pos='n'`).\n",
        "    *   Even when the lemma is the same across different POS tags (e.g., 'bat' as noun or verb both lemmatize to 'bat'), specifying the POS tag guides the lemmatizer to check within the correct grammatical category.\n",
        "*   **Full Preprocessing Pipeline Demonstration**:\n",
        "    *   A complete pipeline successfully demonstrated tokenization, POS tagging, WordNet POS conversion, and lemmatization.\n",
        "    *   Example: The sentence \"The quick brown foxes are running quickly through the beautiful green forests.\" was transformed into `['The', 'quick', 'brown', 'fox', 'be', 'run', 'quickly', 'through', 'the', 'beautiful', 'green', 'forest', '.']`.\n",
        "    *   The process highlighted the need for appropriate NLTK resource downloads (`punkt`, `averaged_perceptron_tagger`, `wordnet`) for a seamless pipeline execution.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   For optimal lemmatization accuracy, especially in complex NLP tasks, it is crucial to integrate a robust POS tagging step into the text preprocessing pipeline to supply the `WordNetLemmatizer` with accurate grammatical context.\n",
        "*   Consider the trade-off between lemmatization accuracy and computational efficiency; while lemmatization is more precise, stemming might be sufficient for tasks where speed is paramount and a perfect base form isn't strictly necessary.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}